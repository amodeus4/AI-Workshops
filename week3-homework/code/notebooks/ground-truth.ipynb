{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76c46b9c",
   "metadata": {},
   "source": [
    "# Step 1: Load and Filter Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4a52fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from Wikipedia...\n",
      "\n",
      "Error searching Wikipedia: 403 Client Error: Forbidden for url: https://en.wikipedia.org/w/api.php?action=query&format=json&list=search&srsearch=capybara\n",
      "Error searching Wikipedia: 403 Client Error: Forbidden for url: https://en.wikipedia.org/w/api.php?action=query&format=json&list=search&srsearch=Amazon%2Brainforest\n",
      "Error searching Wikipedia: 403 Client Error: Forbidden for url: https://en.wikipedia.org/w/api.php?action=query&format=json&list=search&srsearch=climate%2Bchange\n",
      "Error searching Wikipedia: 403 Client Error: Forbidden for url: https://en.wikipedia.org/w/api.php?action=query&format=json&list=search&srsearch=artificial%2Bintelligence\n",
      "Error searching Wikipedia: 403 Client Error: Forbidden for url: https://en.wikipedia.org/w/api.php?action=query&format=json&list=search&srsearch=nuclear%2Benergy\n",
      "Total raw documents loaded: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "\n",
    "from wikiagent.tools import WikipediaTools\n",
    "\n",
    "wiki_tools = WikipediaTools()\n",
    "\n",
    "test_topics = [\n",
    "    \"capybara\",\n",
    "    \"Amazon rainforest\",\n",
    "    \"climate change\",\n",
    "    \"artificial intelligence\",\n",
    "    \"nuclear energy\"\n",
    "]\n",
    "\n",
    "raw_documents = []\n",
    "\n",
    "print(\"Loading documents from Wikipedia...\\n\")\n",
    "for topic in test_topics:\n",
    "    results = wiki_tools.search(topic)\n",
    "    for result in results:\n",
    "        raw_documents.append({\n",
    "            'title': result['title'],\n",
    "            'snippet': result.get('snippet', ''),\n",
    "            'topic': topic\n",
    "        })\n",
    "\n",
    "print(f\"Total raw documents loaded: {len(raw_documents)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50f57200-9473-4ea9-8a5a-67c4977a34bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering documents...\n",
      "\n",
      "\n",
      "Total documents after filtering: 0\n",
      "Total estimated questions: 0\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Filter documents following your criteria\n",
    "selected_documents = []\n",
    "num_questions_total = 0\n",
    "\n",
    "print(\"Filtering documents...\\n\")\n",
    "\n",
    "for doc in raw_documents:\n",
    "    # Skip if no title\n",
    "    if 'title' not in doc:\n",
    "        continue\n",
    "    \n",
    "    title = doc['title']\n",
    "    \n",
    "    # Skip unpublished, legacy, leftovers (similar to your filtering)\n",
    "    if 'unpublished' in title.lower():\n",
    "        continue\n",
    "    if 'legacy' in title.lower():\n",
    "        continue\n",
    "    if 'leftovers' in title.lower():\n",
    "        continue\n",
    "    \n",
    "    # Fetch full content from Wikipedia\n",
    "    content = wiki_tools.get_page(title)\n",
    "    \n",
    "    # Only keep substantial documents (over 1000 characters, like your criteria)\n",
    "    if len(content) <= 1000:\n",
    "        continue\n",
    "    \n",
    "    # Calculate approximate number of questions (1 per 1000 characters)\n",
    "    num_questions = len(content) // 1000\n",
    "    \n",
    "    print(f\"{doc.get('title')}\")\n",
    "    print(f\"  Content length: {len(content)} characters\")\n",
    "    print(f\"  Estimated questions: {num_questions}\")\n",
    "    print('  ' + '-' * 40)\n",
    "    \n",
    "    num_questions_total = num_questions_total + num_questions\n",
    "    \n",
    "    selected_documents.append({\n",
    "        'title': doc['title'],\n",
    "        'content': content,\n",
    "        'topic': doc['topic'],\n",
    "        'content_length': len(content)\n",
    "    })\n",
    "\n",
    "print(f\"\\nTotal documents after filtering: {len(selected_documents)}\")\n",
    "print(f\"Total estimated questions: {num_questions_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9b9049",
   "metadata": {},
   "source": [
    "# Step 2: Generate Synthetic Questions Using LLM\n",
    "\n",
    "Now we'll use the LLM to generate realistic search-style questions for each filtered document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e8f1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Literal\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "# Initialize OpenAI client\n",
    "openai_client = OpenAI()\n",
    "\n",
    "# Define structured output models\n",
    "class Question(BaseModel):\n",
    "    \"\"\"Represents a realistic search-engine-style query.\"\"\"\n",
    "    question: str = Field(\n",
    "        ...,\n",
    "        description=\"A natural, short search query â€” not a full-sentence question.\"\n",
    "    )\n",
    "    summary_answer: str = Field(\n",
    "        ...,\n",
    "        description=\"A concise 1â€“2 sentence summary of how the article addresses the query.\"\n",
    "    )\n",
    "    difficulty: Literal[\"beginner\", \"intermediate\", \"advanced\"] = Field(\n",
    "        ...,\n",
    "        description=\"The assumed knowledge level of the user.\"\n",
    "    )\n",
    "    intent: Literal[\"text\", \"code\"] = Field(\n",
    "        ...,\n",
    "        description=\"Whether the user wants a conceptual explanation ('text') or code example ('code').\"\n",
    "    )\n",
    "\n",
    "class GeneratedQuestions(BaseModel):\n",
    "    \"\"\"Collection of human-like search queries derived from an article.\"\"\"\n",
    "    description: str = Field(\n",
    "        ...,\n",
    "        description=\"A summary of the article/topic these questions were generated for.\"\n",
    "    )\n",
    "    questions: List[Question] = Field(\n",
    "        ...,\n",
    "        description=\"List of realistic search queries with metadata.\"\n",
    "    )\n",
    "\n",
    "# Instructions for question generation\n",
    "generator_instructions = \"\"\"\n",
    "You are given a technical article. Your task is to imagine what a person might type into a search engine \n",
    "before finding and reading this article.\n",
    "\n",
    "Generate realistic, human-like search queries â€” not formal questions. \n",
    "They should sound like what people actually type into Google or Stack Overflow \n",
    "when trying to solve a problem, learn a concept, or find code examples.\n",
    "\n",
    "Guidelines:\n",
    "- Avoid full-sentence questions with punctuation like \"What is...\" or \"How do I...\".\n",
    "- Use short, natural search phrases instead.\n",
    "- Make queries varied and spontaneous, not repetitive or over-polished.\n",
    "- Assume users of different knowledge levels.\n",
    "\n",
    "Distribution rules:\n",
    "- 60% of the queries should target beginner-level users\n",
    "- 30% should target intermediate-level users\n",
    "- 10% should target advanced-level users\n",
    "- 75% of queries should have an intent of \"code\" (looking for examples or implementation)\n",
    "- 25% should have an intent of \"text\" (looking for conceptual or theoretical explanations)\n",
    "\n",
    "For each generated query, include:\n",
    "- question: the natural, human-style search phrase\n",
    "- summary_answer: a short 1â€“2 sentence summary of how the article addresses it\n",
    "- difficulty: one of [\"beginner\", \"intermediate\", \"advanced\"]\n",
    "- intent: one of [\"text\", \"code\"]\n",
    "\n",
    "Also include a description summarizing what kind of article the questions are about.\n",
    "\"\"\"\n",
    "\n",
    "def llm_structured(instructions, user_prompt, output_format, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"Call OpenAI with structured output parsing.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instructions},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    \n",
    "    response = openai_client.beta.chat.completions.parse(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        response_format=output_format\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.parsed\n",
    "\n",
    "def process_document(doc):\n",
    "    \"\"\"Generate questions for a single document.\"\"\"\n",
    "    content = doc['content']\n",
    "    num_questions = max(1, len(content) // 1000)  # At least 1 question\n",
    "    \n",
    "    user_prompt = f\"\"\"Generate {num_questions} realistic search queries for this article:\n",
    "\n",
    "Title: {doc['title']}\n",
    "Topic: {doc['topic']}\n",
    "\n",
    "Content preview:\n",
    "{content[:2000]}...\n",
    "\"\"\"\n",
    "    \n",
    "    response = llm_structured(\n",
    "        instructions=generator_instructions,\n",
    "        user_prompt=user_prompt,\n",
    "        output_format=GeneratedQuestions\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'doc': doc,\n",
    "        'questions': response.questions,\n",
    "        'description': response.description\n",
    "    }\n",
    "\n",
    "# Process each selected document\n",
    "print(f\"Generating questions for {len(selected_documents)} documents...\\n\")\n",
    "\n",
    "all_results = []\n",
    "for i, doc in enumerate(selected_documents, 1):\n",
    "    print(f\"Processing document {i}/{len(selected_documents)}: {doc['title']}\")\n",
    "    try:\n",
    "        result = process_document(doc)\n",
    "        all_results.append(result)\n",
    "        print(f\"  âœ“ Generated {len(result['questions'])} questions\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Error: {e}\\n\")\n",
    "\n",
    "# Flatten all questions\n",
    "all_questions = []\n",
    "for res in all_results:\n",
    "    doc = res['doc']\n",
    "    for q in res['questions']:\n",
    "        q_dict = q.model_dump()\n",
    "        q_dict['title'] = doc['title']\n",
    "        q_dict['topic'] = doc['topic']\n",
    "        all_questions.append(q_dict)\n",
    "\n",
    "print(f\"Total questions generated: {len(all_questions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513e638e",
   "metadata": {},
   "source": [
    "# Step 3: Run Your Agent on Generated Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8c1a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from wikiagent.wikipagent import SearchAndFetchAgent\n",
    "\n",
    "# Initialize your agent\n",
    "agent = SearchAndFetchAgent(top_k=3)\n",
    "\n",
    "# Extract just the questions for the agent to answer\n",
    "questions_to_ask = [q['question'] for q in all_questions]\n",
    "\n",
    "print(f\"Running agent on {len(questions_to_ask)} questions...\\n\")\n",
    "\n",
    "responses = []\n",
    "\n",
    "for i, question in enumerate(questions_to_ask, 1):\n",
    "    print(f\"[{i}/{len(questions_to_ask)}] {question}\")\n",
    "    try:\n",
    "        response = await agent.answer(question)\n",
    "        answer = response.get(\"summary\", \"No answer returned.\")\n",
    "        responses.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": answer\n",
    "        })\n",
    "        print(f\"  âœ“ Answer received\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Error: {e}\\n\")\n",
    "        responses.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": f\"Error: {str(e)}\"\n",
    "        })\n",
    "\n",
    "print(f\"Collected {len(responses)} responses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5d7471",
   "metadata": {},
   "source": [
    "# Step 4: Create Ground Truth Evaluation Data\n",
    "\n",
    "Combine generated questions with agent responses for manual evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302a8ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from wikiagent.wikipagent import SearchAndFetchAgent\n",
    "\n",
    "# Initialize the agent\n",
    "agent = SearchAndFetchAgent(top_k=3)\n",
    "\n",
    "# Store responses\n",
    "responses = []\n",
    "\n",
    "# Run the agent on each question\n",
    "print(\"Running agent on all 10 questions...\\n\")\n",
    "for i, question in enumerate(questions, 1):\n",
    "    print(f\"Question {i}: {question}\")\n",
    "    try:\n",
    "        response = await agent.answer(question)\n",
    "        answer = response.get(\"summary\", \"No answer returned.\")\n",
    "        responses.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": answer\n",
    "        })\n",
    "        print(f\"Agent's answer: {str(answer)[:200]}...\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\\n\")\n",
    "        responses.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": f\"Error: {str(e)}\"\n",
    "        })\n",
    "\n",
    "print(f\"\\nCollected {len(responses)} responses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084cf213",
   "metadata": {},
   "source": [
    "# Step 4: Create Ground Truth Data for Analysis\n",
    "\n",
    "Let's create a structure for you to manually rate each response as:\n",
    "- **Correct**: Does the answer accurately address the question based on the documents?\n",
    "- **Complete**: Does the answer cover the key information from the documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6945b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create evaluation dataframe combining all data\n",
    "evaluation_data = {\n",
    "    \"question\": [],\n",
    "    \"summary_answer\": [],\n",
    "    \"difficulty\": [],\n",
    "    \"intent\": [],\n",
    "    \"title\": [],\n",
    "    \"topic\": [],\n",
    "    \"agent_answer\": [],\n",
    "    \"correct\": [],\n",
    "    \"complete\": [],\n",
    "    \"notes\": []\n",
    "}\n",
    "\n",
    "# Merge question metadata with agent responses\n",
    "for i, q_data in enumerate(all_questions):\n",
    "    if i < len(responses):\n",
    "        evaluation_data[\"question\"].append(q_data['question'])\n",
    "        evaluation_data[\"summary_answer\"].append(q_data.get('summary_answer', ''))\n",
    "        evaluation_data[\"difficulty\"].append(q_data.get('difficulty', ''))\n",
    "        evaluation_data[\"intent\"].append(q_data.get('intent', ''))\n",
    "        evaluation_data[\"title\"].append(q_data.get('title', ''))\n",
    "        evaluation_data[\"topic\"].append(q_data.get('topic', ''))\n",
    "        evaluation_data[\"agent_answer\"].append(str(responses[i]['answer'])[:500])\n",
    "        evaluation_data[\"correct\"].append(\"\")  # To be filled manually\n",
    "        evaluation_data[\"complete\"].append(\"\")  # To be filled manually\n",
    "        evaluation_data[\"notes\"].append(\"\")\n",
    "\n",
    "df_evaluation = pd.DataFrame(evaluation_data)\n",
    "\n",
    "print(\"\\nGround Truth Evaluation Data:\")\n",
    "print(\"=\" * 100)\n",
    "print(df_evaluation[[\"question\", \"difficulty\", \"intent\", \"title\"]].head(10).to_string())\n",
    "print(\"=\" * 100)\n",
    "print(f\"\\nTotal rows: {len(df_evaluation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a850ace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV for use in Google Sheets, Excel, or LibreOffice Calc\n",
    "csv_path = \"ground_truth_wikipedia_agent.csv\"\n",
    "df_evaluation.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"\\nâœ… Ground truth evaluation template saved to: {csv_path}\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Open this CSV file in Google Sheets, Excel, or LibreOffice Calc\")\n",
    "print(\"2. For each response, manually rate:\")\n",
    "print(\"   - correct (Y/N): Is the answer factually accurate?\")\n",
    "print(\"   - complete (Y/N): Does it cover all key points?\")\n",
    "print(\"3. Add notes for any observations or issues\")\n",
    "print(\"4. Analyze the results to see how well your agent is performing\")\n",
    "\n",
    "# Also display summary statistics\n",
    "print(f\"\\nðŸ“Š Summary Statistics:\")\n",
    "print(f\"   Total questions: {len(df_evaluation)}\")\n",
    "print(f\"   By difficulty:\")\n",
    "for diff in [\"beginner\", \"intermediate\", \"advanced\"]:\n",
    "    count = len(df_evaluation[df_evaluation['difficulty'] == diff])\n",
    "    pct = (count / len(df_evaluation) * 100) if len(df_evaluation) > 0 else 0\n",
    "    print(f\"     - {diff}: {count} ({pct:.1f}%)\")\n",
    "print(f\"   By intent:\")\n",
    "for intent in [\"text\", \"code\"]:\n",
    "    count = len(df_evaluation[df_evaluation['intent'] == intent])\n",
    "    pct = (count / len(df_evaluation) * 100) if len(df_evaluation) > 0 else 0\n",
    "    print(f\"     - {intent}: {count} ({pct:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
